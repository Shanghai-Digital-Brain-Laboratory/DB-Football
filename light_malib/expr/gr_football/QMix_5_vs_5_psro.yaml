expr_group: gr_football
expr_name: 5_v_5_qmix
log_dir: ./logs             # log directory
seed: 0
eval_only: False            # True if only want evaluation

distributed:
  use: False
  auto_connect:
  auto_copy:
  nodes:
    master:
      ip: "auto"
    workers:      
      - ip:

framework:
  name: "psro"                # framework name
  max_generations: 50         # psro rounds
  meta_solver: "nash"         # 'nash', 'uniform', 'pfsp'
  sync_training: False
  stopper:
    type: "win_rate_stopper"
    kwargs:
      min_win_rate: 1
      max_steps: 10000          # iteration at each psro round
agent_manager:
  num_agents: 2
  share_policies: True

evaluation_manager:
  num_eval_rollouts: 1

policy_data_manager:
  update_func: "gr_football" 
  fields:
    payoff:
      type: "matrix"
      missing_value: -100 
    score:
      type: "matrix"
      missing_value: -100 
    win:
      type: "matrix"
      missing_value: -100 
    lose:
      type: "matrix"
      missing_value: -100 
    my_goal:
      type: "matrix"
      missing_value: -100 
    goal_diff:
      type: "matrix"
      missing_value: -100 
        
monitor:
  type: "local"
  distributed:
    resources:
      num_cpus: 1
      resources:
        - ["node:${distributed.nodes.master.ip}",0.01]

rollout_manager:
  distributed:
    resources:
      num_cpus: 1
      resources:
        - ["node:${distributed.nodes.master.ip}",0.01]
  num_workers: 100   # numbers of parallel rollout worker, one env per worker by default
  seed: 12345
  saving_interval: 100 # the frequency of dumping model's weight
  batch_size: ${rollout_manager.num_workers}
  eval_batch_size: 100
  eval_freq: 100 # epochs
  min_samples: ${training_manager.batch_size}
  rollout_metric_cfgs:
    reward:
      type: "sliding"
      window_size: 20
      init_list: [-10000]
    win:
      type: "sliding"
      window_size: 20
      init_list: [0,0,0,0,0]
  worker:
    distributed:
      resources:
        num_cpus: 1
    rollout_length: 3001                                    # episode length
    eval_rollout_length: 3001
    sample_length: 1000                                     # every $sample_length traj will be push to buffer during rollout
    padding_length:                                         # of not use in gr_football
    rollout_func_name: "rollout_func"
    episode_mode: 'traj'
    envs:
      - cls: "gr_football"
        id_prefix: "gr_footbal"
        scenario_config:
          env_name: "5_vs_5"
          number_of_left_players_agent_controls: 4
          number_of_right_players_agent_controls: 4
          representation: "raw"
          rewards: "scoring, checkpoints"
          stacked: False
          logdir: '/tmp/football/malib_psro'
          write_goal_dumps: False
          write_full_episode_dumps: False
          render: False
          other_config_options:
            action_set: v2  #default/v2, v2 is only used for built-in AI
        reward_config:
          win_reward: 0
          preprocess_score: 0
          ball_position_reward: 0
          yellow_reward: 0
          min_dist_reward: 0
          goal_reward: 5
          #pass_reward: 0.02
          shot_reward: 0
          lost_ball_reward: 0.
          player_move_reward: 0.
          dist_goal_to_line: 0
          role_based_r: 0
          pure_goal: 0
          pure_lose_goal: 0
    credit_reassign:
      goal: 0.2
      assist: 0.2
      loseball: 0.05
      halt_loseball: 0.05
      gainball: 0
    decaying_exploration:
      init_noise: 0             #random exploration noise level
      total_epoch_to_zero: 2000    #number of epoch when exploration noise decay to zero
      interval: 400                #number of epoch at each fixed exploration noise level

training_manager:
  distributed:
    resources:
      num_cpus: 1
      resources:
        - ["node:${distributed.nodes.master.ip}",0.01]
 
  master_addr: "127.0.0.1"
  master_port:  "12774"
  local_queue_size: 1
  batch_size: 80 # how many data sample from DatasetServer per time.
  num_prefetchers: 1
  data_prefetcher:
    distributed:
      resources:
        num_cpus: 1
  num_trainers: 1 # equals to number of GPUs by default
  # control the frequency of remote parameter update
  update_interval: 1
  gpu_preload: False
  trainer:
    distributed:
      resources:
        num_cpus: 1
        num_gpus: 1
        resources:
          - ["node:${distributed.nodes.master.ip}",0.01]
    optimizer: "Adam"
    actor_lr: 5.e-4
    critic_lr: 5.e-4
    opti_eps: 1.e-5
    weight_decay: 0.0
    lr_decay: False            #update_linear_schedule
    lr_decay_epoch: 2000      #how many rollout steps till zero
    use_max_grad_norm: True
    max_grad_norm: 0.2

data_server:
  distributed:
    resources:
      num_cpus: 1
      resources:
        - ["node:${distributed.nodes.master.ip}",0.01]
  table_cfg:
    capacity: 2000
    sampler_type: "uniform"
    sample_max_usage: 10000
    rate_limiter_cfg:
      min_size: ${training_manager.batch_size}
      # r_w_ratio: 10.0
  read_timeout: 120

policy_server:
  distributed:
    resources:
      num_cpus: 1
      resources:
        - ["node:${distributed.nodes.master.ip}",0.01]

populations:
  - population_id: default # population_id
    algorithm:
      name: "QMix"
      model_config:
        model: "gr_football.qmix_5_v_5"       # model type
        hidden_size: 128
        central_obs_dim: 115
        initialization:
        actor:

      # set hyper parameter
      custom_config:
        use_cuda: False  # enable cuda or not
        prev_act_inp: False
        num_mini_batch: 1
        local_q_config:
          n_agent: 4          #1 if parameter-sharing
          use_orthogonal: True
          hidden_size: 128
          use_rnn_layer: False
          gain: 1.
          use_feature_normalization: True
          use_ReLU: True
          use_conv1d: False
          stacked_frames: False
          layer_N: 2
        qmixer_config:
          n_agent: 4
          mixer_embed_dim: 128
          hyper_hidden_dim: 128

      policy_init_cfg:
        agent_0: # agent_id
          new_policy_ctr_start: -1
          init_cfg:
            - condition: "==0" # condition if True in order
              strategy: random # now support pretrained, inherit_last, random, inherit_last_best
              policy_id: 
              policy_dir:
            - condition: "default" # default case.
              strategy: inherit_last_best # now support pretrained, inherit_last, random, inherit_last_best
              policy_id:
              policy_dir:
          initial_policies:                         #initial population
            - policy_id: built_in_5
              policy_dir: light_malib/trained_models/gr_football/5_vs_5/built_in
#            - policy_id: current_best
#              policy_dir: light_malib/trained_models/gr_football/11_vs_11/current_best
#            - policy_id: defensive_passer
#              policy_dir: light_malib/trained_models/gr_football/11_vs_11/defensive_passer
#            - policy_id: offensive_passer
#              policy_dir: light_malib/trained_models/gr_football/11_vs_11/offensive_passer
#            - policy_id: group_pressure
#              policy_dir: light_malib/trained_models/gr_football/11_vs_11/group_pressure